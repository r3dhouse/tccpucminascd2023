# -*- coding: utf-8 -*-
"""base_aplic_ml.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16bmq9IkH8ArqdVzeNxoPR9fdtuHlikJP

# 1) Preparação do Ambiente

## 1.1) Instalação das Bibliotecas de Machine Learning:

  *   Catboost;
  *   XGBoost.
"""

print('---***'*60)
print('---***'*60)
!pip install catboost
print('---***'*60)
print('---***'*60)
!pip install xgboost
print('---***'*60)
print('---***'*60)

"""## 1.2) Importação das Bibliotecas

          a)   Numpy;
          b)   Pandas;
          c)   Matplotlib;
          d)   Seaborn;
          e)   Catboost;
          f)   XGBoost;
          g)   KNeighbors.


"""

# coding=utf-8

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from catboost import CatBoostRegressor, Pool
from xgboost import XGBRegressor

from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV

from google.colab import files

pd.set_option('display.precision', 3)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

"""## 1.3) Carga da Base Final para aplicação dos modelos de Machine Learning(Arquivo Excel)

Base contém os dados conjuntos de Arrecadação, PIB e Inadimplência
*   Carga do arquivo base_final_ml.xlsx para Dataframe base
"""

arq_base = files.upload()
colunas = 'B:AG'

base = pd.read_excel(arq_base['base_final_ml.xlsx'], usecols=colunas)

"""## 1.5) Estatística Descritiva dos Dados

*   Campos;
*   Tipos dos campos;
*   Atributos categóricos;
*   Descrição dos dados;
"""

print('*' * 60)
print('Campos da Base Inadimplência, Arrecadação e PIB:')
print('*' * 60)
print('-' * 31)
print('Colunas          Tipo dos dados')
print('-' * 31)
print(base.dtypes)
print('*' * 100)
print('Descrição dos dados:')
print('-' * 73)
print(base.describe())
print('*' * 100)

"""# -----------------------------------------------------------------------------------------------------------

# 2) Bases de Treinamento e Teste e Definição do Baseline

O valor da média para a base_processamento da Inadimplência de 2020 ('inad_2020') ficou em 0,572.

Usaremos o método mean_absolute_error para calcular o Baseline ou valor de referência que será o MAE - Erro Absoluto Médio.

## 2.1) Divisão da base em:

*   Treinamento;
*   Teste.
"""

X_aux = base [:]
X = X_aux.copy()
X.drop(columns=['inad_2020'], inplace=True)
y = base [['inad_2020']]

print('*' * 300)
print('Variáveis preditoras:')
print('-'* 21)
print(X)
print('*' * 300)
print('*' * 30)
print('Label:')
print('-' * 30)
print(y)
print('*' * 30)

X_treinamento, X_teste, y_treinamento, y_teste = train_test_split(X, y, random_state=0)

print('Base Treinamento:')
print(X_treinamento)
print('Rótulo Treinamento')
print(y_treinamento)
print('***' * 30)
print('Base Teste:')
print(X_teste)
print('Rótulo Teste')
print(y_teste)

"""## 2.2) Preparação da Baseline"""

print('*' * 60)
print('Comparativo das Médias da Inadimplência de 2020 das bases')
print('*' * 60)
print('-' * 30)
print('Base completa (y):')
print(y.mean()[0])
print('-' * 30)
print('Base de treinamento (y_treinamento):')
print(y_treinamento.mean()[0])
print('-' * 30)
print('Base de teste (y_teste):')
print(y_teste.mean()[0])
print('-' * 30)
vmr_base_compl = y.mean()[0]

# Criando ndarray replicando a média para o nº de linhas da base completa
y_pred = np.empty(len(y))
y_pred.fill(vmr_base_compl)

"""## 2.3) Cálculo da Baseline"""

def resultados (real, predicao, comparar):
    print(comparar)
    print('MAE: ', mean_absolute_error(real, predicao))

resultados(y, y_pred, 'Baseline')

"""# 3) Aplicação dos Modelos de Machine Learning

## 3.1) Gráfico de dispersão da predição versus real
"""

def graf_dispersao(real, predicao):
    plt.scatter(real, predicao)
    amplitude = [real.min(), predicao.max()]
    plt.plot(amplitude, amplitude, 'green')
    plt.xlabel('Inadimplência real de 2020')
    plt.ylabel('Inadimplência predita de 2020')
    plt.show()

"""## 3.2) Nearest Neighbors Regression"""

modelo_knn = KNeighborsRegressor()
modelo_knn.fit(X_treinamento, y_treinamento)

pred_knn = modelo_knn.predict(X_teste)

resultados(y_teste, pred_knn, 'Nearest Neighbors Regression')
label_real = y_teste.to_numpy()
graf_dispersao(label_real, pred_knn)

"""## 3.3) XGBoost"""

modelo_xgboost = XGBRegressor()
modelo_xgboost.fit(X_treinamento, y_treinamento)

pred_xgboost = modelo_xgboost.predict(X_teste)

resultados(y_teste, pred_xgboost, 'XGBoost')
lbl_real_xgb = y_teste.to_numpy()
graf_dispersao(lbl_real_xgb, pred_xgboost)

"""## 3.4) CatBoost"""

treinamento_pool = Pool(X_treinamento, y_treinamento)
teste_pool = Pool(X_teste, y_teste)

modelo_catboost = CatBoostRegressor(loss_function='MAE')
modelo_catboost.fit(treinamento_pool)

pred_catboost = modelo_catboost.predict(teste_pool)

resultados(y_teste, pred_catboost, 'Catboost')
lbl_real_cb = y_teste.to_numpy()
graf_dispersao(lbl_real_cb, pred_catboost)

"""# 4) Análise de Resíduos

## 4.1) Análise de Resíduos para o Modelo de Melhor Performance ==>> CatBoost
"""

ar_pool = Pool(X, y)

ar_modelo_catboost = CatBoostRegressor(loss_function='MAE')
ar_modelo_catboost.fit(ar_pool)

ar_pred_catboost = ar_modelo_catboost.predict(ar_pool)

resultados(y, ar_pred_catboost, 'Catboost - Análise de Resíduos')

"""## 4.2) Gráfico da Inadimplência 2020"""

def plota_box (dados, coluna):
    sns.boxplot(data=dados[coluna], orient="v")
    plt.show()
    # dados.boxplot(column=coluna, figsize=(9,9))
    print('Índice  Valor')
    print(dados[coluna].nlargest(n=5))
    print(dados[coluna].agg(['min', 'max']))
    print(dados[coluna].describe())
    print('*' * 150)

X_auxiliar = X [:]
X_an_res = X_auxiliar.copy()

X_an_res['inad_predicao'] = ar_pred_catboost

X_an_res['residuo'] = abs(base['inad_2020'] - X_an_res['inad_predicao'])

# Boxplot e Histograma da análise de resíduos
plota_box(X_an_res,'residuo')

X_an_res.hist(column='residuo', figsize=(9,6), bins=5)

"""# 8) Hiperparâmetros e Cross Validation"""

modelo_cv = CatBoostRegressor(loss_function='MAE')

grid = {'max_depth': [3,4,5,6,7,8,9,10],'n_estimators':[100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100]}

gscv = GridSearchCV (estimator = modelo_cv, param_grid = grid, scoring ='neg_mean_absolute_error', cv = 5)

gscv.fit(X, y)

print(gscv.best_estimator_)
print(gscv.best_score_)
print(gscv.best_params_)

"""## 8.2) Aplicação do Cross Validation e Hiperparâmetros"""

cv_hp_pool = Pool(X, y)
modelo_cv_hp = CatBoostRegressor(loss_function='MAE', max_depth=7, n_estimators=1100)

modelo_cv_hp.fit(cv_hp_pool)

cv_hp_pred_catboost = modelo_cv_hp.predict(cv_hp_pool)

resultados(y, cv_hp_pred_catboost, 'Catboost - Grid Search CV e HP')

modelo_cv_rs = CatBoostRegressor(loss_function='MAE')

param_dist = {'max_depth': [3,4,5,6,7,8,9,10],'n_estimators':[100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100]}

rscv = RandomizedSearchCV(modelo_cv_rs, param_dist, scoring ='neg_mean_absolute_error', cv = 5)

rscv.fit(X, y)

print(rscv.best_estimator_)
print(rscv.best_score_)
print(rscv.best_params_)

rs_cv_hp_pool = Pool(X, y)
modelo_rs_cv_hp = CatBoostRegressor(loss_function='MAE', max_depth=10, n_estimators=500)

modelo_rs_cv_hp.fit(rs_cv_hp_pool)

rs_cv_hp_pred_catboost = modelo_rs_cv_hp.predict(rs_cv_hp_pool)

resultados(y, rs_cv_hp_pred_catboost, 'Catboost - Random Search - CV e HP')